{
    "root": {
        "cell_type": {
            "3582ccd3": "code",
            "36e9880b": "code",
            "70facad6": "code",
            "6249a7fc": "code",
            "c981b9f1": "code",
            "baa12dff": "code",
            "76f795db": "code",
            "564335ad": "code",
            "485d331f": "code",
            "5db73451": "markdown",
            "fe0744d8": "code",
            "52e10ab7": "code",
            "ccbd8d75": "code",
            "fbfb5dc0": "code",
            "dd85de06": "markdown",
            "708aa9e2": "code"
        },
        "source": {
            "3582ccd3": "import re\nimport csv\nimport nltk\nfrom nltk import ngrams\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem.lancaster import LancasterStemmer\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom num2words import num2words\n\n# TASK 2\n\n# Read input from app descriptions csv file\nwith open('MyAppDescriptions.csv') as csv_file:\n    csv_reader = csv.reader(csv_file, delimiter=',')\n    line_count = 0\n    app_desc_csv=[]\n    app_names_csv=[]\n    for row in csv_reader:\n        if line_count == 0:\n            col_names = row\n            line_count += 1\n        else:\n            app_desc_csv.append(row[3])\n            app_names_csv.append(row[1])\n            line_count += 1\napp_desc_list=[]\n\n# Iterate through app descriptions\nfor desc in app_desc_csv:\n    app_desc = desc\n    # Initialize punctuations string\n    punc = '''!()-[]{};:'\"...\\,./?@*'''\n    # Remove punctuations in description\n    for ch in app_desc:\n        if ch in punc:\n            app_desc = app_desc.replace(ch, \"\")\n    app_desc_list.append(app_desc)\n    \nfor wd in app_desc_list:\n    print(wd)\n    print('')        ",
            "36e9880b": "for desc in app_desc_list:\n    # Remove any non words or  white spaces\n    app_desc_sc = re.sub(r'[^\\w\\s]', '', desc)\n\n    # Emoji pattern\n    emoji_pattern = re.compile(\"[\"\n            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               \"]+\", flags=re.UNICODE)\n\n    # Remove emojis in description\n    emoji_pattern.sub(r'', app_desc_sc)\n    app_desc_list[app_desc_list.index(desc)]=app_desc_sc\n    \nfor wd in app_desc_list:\n    print(wd)\n    print('')  ",
            "70facad6": "for desc in app_desc_list:\n    app_desc_num=desc\n    # Initialize list of words in description\n    word_tokens_num = word_tokenize(app_desc_num)\n\n    # Iterate list and change numbers to words\n    for i in word_tokens_num:\n        if i.isdigit():\n            app_desc_num = app_desc_num.replace(i, num2words(i))\n\n    app_desc_list[app_desc_list.index(desc)]=app_desc_num\nfor wd in app_desc_list:\n    print(wd)\n    print('')        ",
            "6249a7fc": "for desc in app_desc_list:\n    app_desc_sp = desc\n    # Remove extra whitespaces using regex string\n    app_desc_sp = re.sub(' {2,}', ' ', app_desc_sp)\n    app_desc_list[app_desc_list.index(desc)]=app_desc_sp\nfor wd in app_desc_list:\n    print(wd)\n    print('')        ",
            "c981b9f1": "for desc in app_desc_list:\n    app_desc_low=desc\n    # Initialize list of words in description\n    word_tokens_low = word_tokenize(app_desc_low)\n\n    # Iterate list and change words to lowercase\n    for i in word_tokens_low:\n        app_desc_low = app_desc_low.replace(i, i.lower())\n    app_desc_list[app_desc_list.index(desc)]=app_desc_low\nfor wd in app_desc_list:\n    print(wd)\n    print('')     ",
            "baa12dff": "for desc in app_desc_list:\n#   Initialize set of english stop words\n    stop_words = set(stopwords.words('english'))\n    word_tokens_st = word_tokenize(desc)\n#   Initialize list with stop words filtered out and initialize new app description string\n    filtered_desc = [w for w in word_tokens_st if not w in stop_words]\n    app_desc_stop=' '.join([str(elem) for elem in filtered_desc])\n    app_desc_list[app_desc_list.index(desc)]=app_desc_stop\nfor wd in app_desc_list:\n    print(wd)\n    print('')  ",
            "76f795db": "for desc in app_desc_list:\n#   Initialize stemmer object \n    snowball_stemmer  = SnowballStemmer('english')\n    app_desc_stem = desc\n    word_tokens_stem = word_tokenize(desc)\n#   Iterate word tokens and stem words\n    for w in word_tokens_stem:\n        app_desc_stem = app_desc_stem.replace(w, snowball_stemmer.stem(w))\n    app_desc_list[app_desc_list.index(desc)]=app_desc_stem\nfor wd in app_desc_list:\n    print(wd)\n    print('')  \n",
            "564335ad": "for desc in app_desc_list:\n#   Initialize Lemmatizer object \n    wordnet_lemmatizer = WordNetLemmatizer()\n    app_desc_lem = desc\n    word_tokens_lem = word_tokenize(desc)\n    \n#   Iterate word tokens and Lemmatize words\n    for w in word_tokens_lem:\n        app_desc_lem = app_desc_lem.replace(w, wordnet_lemmatizer.lemmatize(w))\n    app_desc_list[app_desc_list.index(desc)]=app_desc_lem\nfor wd in app_desc_list:\n    print(wd)\n    print('')\n",
            "485d331f": "# Write output to PreProcessedDescription csv\nwith open('PreProcessedDescription.csv', 'w') as csvfile:\n    fieldnames = ['app_name', 'app_description']\n    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n    writer.writeheader()\n    for desc in app_desc_list:\n        writer.writerow({'app_name': app_names_csv[app_desc_list.index(desc)].strip(), 'app_description': desc})\n   ",
            "5db73451": "# TASK 2 \n\n### X. What is the difference between stemming and lemmatization?\n\n**Stemming** is the process of reducing the inflection in a word by cutting it down to its root *stem* which may or may not be a valid word in the grammar of that language. On the other hand **Lemmatization** is a similar process of reducing the inflection in a word but it cuts the word down to its root *lemma* which is the canonical form of the word so it is still meaningful in the context of the original language.",
            "fe0744d8": "# TASK 3\n\n# Read input from PreProcessedDescription csv file\nwith open('PreProcessedDescription.csv') as csv_file_br:\n    csv_reader_bg = csv.reader(csv_file_br, delimiter=',')\n    line_count_bg = 0\n    app_desc_csv_bg=[]\n    app_names_csv_bg=[]\n    for row in csv_reader_bg:\n        if line_count_bg == 0:\n            col_names_bg = row\n            line_count_bg += 1\n        else:\n            app_desc_csv_bg.append(row[1])\n            app_names_csv_bg.append(row[0])\n            line_count_bg += 1\n# Write to BiGrams csv\nwith open('BiGrams.csv', 'w') as csvfile_bw:\n    fieldnames = ['app_name','Bi_grams']\n    writer = csv.DictWriter(csvfile_bw, fieldnames=fieldnames)\n    writer.writeheader()\n    for desc in app_desc_csv_bg:\n        app_desc = desc\n        bi_grams_list=[]\n        bi_grams = ngrams(app_desc.split(), 2)\n        for grams in bi_grams:  \n            big_str=tuple(grams)[0] +\" \"+ tuple(grams)[1]\n            bi_grams_list.append(big_str)\n        app=app_names_csv_bg[app_desc_csv_bg.index(desc)]\n        print(app.strip(), \" Bi-grams: \")\n        for el in bi_grams_list:\n            print(el)\n            writer.writerow({'app_name': app.strip(), 'Bi_grams':el})\n        print(\"\\n\")\n        ",
            "52e10ab7": "# TASK 4\n\n# Read input from PreProcessedDescription csv file\nwith open('PreProcessedDescription.csv') as csv_file_tr:\n    csv_reader_tg = csv.reader(csv_file_tr, delimiter=',')\n    line_count_tg = 0\n    app_desc_csv_tg=[]\n    app_names_csv_tg=[]\n    for row in csv_reader_tg:\n        if line_count_tg == 0:\n            col_names_tg = row\n            line_count_tg += 1\n        else:\n            app_desc_csv_tg.append(row[1])\n            app_names_csv_tg.append(row[0])\n            line_count_tg += 1\n# Write to TriGrams csv\nwith open('TriGrams.csv', 'w') as csvfile_tw:\n    fieldnames = ['app_name','tri_grams']\n    writer = csv.DictWriter(csvfile_tw, fieldnames=fieldnames)\n    writer.writeheader()\n    for desc in app_desc_csv_tg:\n        app_desc = desc\n        tri_grams_list=[]\n        tri_grams = ngrams(app_desc.split(), 3)\n        for grams in tri_grams:  \n            big_str=tuple(grams)[0] +\" \"+ tuple(grams)[1] +\" \"+ tuple(grams)[2]\n            tri_grams_list.append(big_str)\n       \n        app=app_names_csv_tg[app_desc_csv_tg.index(desc)]\n        print(app.strip(), \" Tri-grams: \")\n        for el in tri_grams_list:\n            print(el)\n            writer.writerow({'app_name': app.strip(), 'tri_grams':el})\n        print(\"\\n\")\n        ",
            "ccbd8d75": "# TASK 5\n\nprint(\"BI-GRAMS\")\nfor app in app_names_csv_tg:\n    # Read input from MyAppFeatures csv file\n    with open('MyAppFeatures.csv') as csv_file_cm:\n        csv_reader_cm = csv.reader(csv_file_cm, delimiter=',')\n        line_count_cm = 0\n        app_feat_list=[]\n        for row in csv_reader_cm:\n            if line_count_cm == 0:\n                col_names_cm = row\n                line_count_cm += 1\n            else:\n                if row[1].strip()==app:\n                    app_feat_list.append(row[2])\n                    line_count_cm += 1\n    \n    with open('BiGrams.csv') as csv_file_b:\n        csv_reader_b = csv.reader(csv_file_b, delimiter=',')\n        line_count_b = 0\n        app_bigrams_list=[]\n        \n        for row in csv_reader_b:\n            if line_count_b == 0:\n                col_names_b = row\n                line_count_b += 1\n            else:\n                if row[0].strip()==app:\n                    app_bigrams_list.append(row[1])\n                    line_count_b += 1\n\n    print(app, \"features substring matching:\")\n    hits=0\n    losses=0\n    for i in app_feat_list:\n \n        for j in app_bigrams_list:\n            if j in i:\n                hits += 1\n                break\n            elif (app_bigrams_list.index(j)==(len(app_bigrams_list)-1)):\n                losses += 1\n    \n    print(\"Hits:\", hits)\n    print(\"Losses:\", losses)\n    print(\"\")\n",
            "fbfb5dc0": "# TASK 5\n\nprint(\"TRI-GRAMS\")\nfor app in app_names_csv_tg:\n    # Read input from MyAppFeatures csv file\n    with open('MyAppFeatures.csv') as csv_file_cm:\n        csv_reader_cm = csv.reader(csv_file_cm, delimiter=',')\n        line_count_cm = 0\n        app_feat_list=[]\n        for row in csv_reader_cm:\n            if line_count_cm == 0:\n                col_names_cm = row\n                line_count_cm += 1\n            else:\n                if row[1].strip()==app:\n                    app_feat_list.append(row[2])\n                    line_count_cm += 1\n    \n    with open('TriGrams.csv') as csv_file_b:\n        csv_reader_b = csv.reader(csv_file_b, delimiter=',')\n        line_count_b = 0\n        app_trigrams_list=[]\n        \n        for row in csv_reader_b:\n            if line_count_b == 0:\n                col_names_b = row\n                line_count_b += 1\n            else:\n                if row[0].strip()==app:\n                    app_trigrams_list.append(row[1])\n                    line_count_b += 1\n\n    print(app, \"features substring matching:\")\n    hits=0\n    losses=0\n    for i in app_feat_list:\n \n        for j in app_trigrams_list:\n            if j in i:\n                hits += 1\n                break\n            elif (app_trigrams_list.index(j)==(len(app_trigrams_list)-1)):\n                losses += 1\n    \n    print(\"Hits:\", hits)\n    print(\"Losses:\", losses)\n    print(\"\")\n",
            "dd85de06": "# Task 5\n\n### III. The extent of similarity and difference between the manually extracted features and the Bi-grams and Tri-grams\n\nI found that the manually extracted features had greater similarity to the Bi-grams than the Tri-grams, this is first because the manually extracted features had substring matching hits in three out of the assigned five apps Bi-grams and none for the Tri-grams. However, it seems Some features couldn't be matched to the extracted n-grams by using a simple substring matching algorithm resulting in losses, but upon manual inspection, the extracted n-grams look very similar to the features that were manually extracted the only differences ranging from the order the words appear to the pre-processing that was done.\n\n### IV. If extracting Bi-grams and Tri-grams are reliable enough for automated extraction of features from app descriptions?\n\nI think only extracting Bi-grams and Tri-grams is not reliable enough for automated extraction of features from apps because to automate the extraction of features as well as being able to extract n-grams from the description the machine should also be able to have a way of recognizing what parts of the description are actually features of the app, one way this can be done is by providing the system a set of patterns that feature description usually follows in natural language so it is able to learn and extract the most relevant features from descriptions. Another reason I don't think it is enough is that when extracting n-grams from text there is going to be an overlap of words so some processing needs to be done to cluster the features and extract a more precise set of features.",
            "708aa9e2": ""
        }
    }
}