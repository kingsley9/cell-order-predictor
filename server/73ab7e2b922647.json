{
    "id": {
        "0": "31f4321d0d1042",
        "1": "31f4321d0d1042",
        "2": "31f4321d0d1042",
        "3": "31f4321d0d1042",
        "4": "31f4321d0d1042",
        "5": "31f4321d0d1042",
        "6": "31f4321d0d1042",
        "7": "31f4321d0d1042",
        "8": "31f4321d0d1042",
        "9": "31f4321d0d1042",
        "10": "31f4321d0d1042",
        "11": "31f4321d0d1042",
        "12": "31f4321d0d1042",
        "13": "31f4321d0d1042",
        "14": "31f4321d0d1042",
        "15": "31f4321d0d1042",
        "16": "31f4321d0d1042",
        "17": "31f4321d0d1042",
        "18": "31f4321d0d1042",
        "19": "31f4321d0d1042",
        "20": "31f4321d0d1042",
        "21": "31f4321d0d1042",
        "22": "31f4321d0d1042",
        "23": "31f4321d0d1042",
        "24": "31f4321d0d1042",
        "25": "31f4321d0d1042",
        "26": "31f4321d0d1042",
        "27": "31f4321d0d1042",
        "28": "31f4321d0d1042",
        "29": "31f4321d0d1042",
        "30": "31f4321d0d1042",
        "31": "31f4321d0d1042",
        "32": "31f4321d0d1042",
        "33": "31f4321d0d1042",
        "34": "31f4321d0d1042",
        "35": "31f4321d0d1042",
        "36": "31f4321d0d1042",
        "37": "31f4321d0d1042",
        "38": "31f4321d0d1042",
        "39": "31f4321d0d1042",
        "40": "31f4321d0d1042",
        "41": "31f4321d0d1042",
        "42": "31f4321d0d1042",
        "43": "31f4321d0d1042",
        "44": "31f4321d0d1042"
    },
    "cell_id": {
        "0": "0ca272a2",
        "1": "f6e9ddc4",
        "2": "021022b0",
        "3": "de007e36",
        "4": "77559638",
        "5": "acc15f6c",
        "6": "57bbad61",
        "7": "70f11e44",
        "8": "b9fe9a9a",
        "9": "7bdf782c",
        "10": "85d048fe",
        "11": "09d6963a",
        "12": "e0fdd1b4",
        "13": "9e76d2d6",
        "14": "b69ffebd",
        "15": "4d9d5e5d",
        "16": "5e01b24e",
        "17": "c1a22072",
        "18": "586e386a",
        "19": "51cb8d65",
        "20": "1f113889",
        "21": "fe41b7ef",
        "22": "55af3457",
        "23": "068df21b",
        "24": "adc25e39",
        "25": "a4848f5c",
        "26": "a48549e5",
        "27": "048f7d27",
        "28": "bd7cd23e",
        "29": "919eed9f",
        "30": "4aa460c9",
        "31": "ed43d90f",
        "32": "469bdfd1",
        "33": "f1f6f1a4",
        "34": "6dc4a4b2",
        "35": "778da2ab",
        "36": "bc18b7b9",
        "37": "6dc700db",
        "38": "233e2934",
        "39": "3e14e659",
        "40": "09c071a9",
        "41": "e02c426a",
        "42": "0012c938",
        "43": "88e1d739",
        "44": "caff8a53"
    },
    "cell_type": {
        "0": "code",
        "1": "markdown",
        "2": "code",
        "3": "code",
        "4": "code",
        "5": "code",
        "6": "code",
        "7": "markdown",
        "8": "code",
        "9": "code",
        "10": "code",
        "11": "code",
        "12": "code",
        "13": "markdown",
        "14": "markdown",
        "15": "markdown",
        "16": "code",
        "17": "markdown",
        "18": "code",
        "19": "markdown",
        "20": "code",
        "21": "markdown",
        "22": "markdown",
        "23": "code",
        "24": "markdown",
        "25": "code",
        "26": "code",
        "27": "code",
        "28": "code",
        "29": "markdown",
        "30": "markdown",
        "31": "code",
        "32": "markdown",
        "33": "code",
        "34": "markdown",
        "35": "code",
        "36": "markdown",
        "37": "code",
        "38": "markdown",
        "39": "markdown",
        "40": "code",
        "41": "markdown",
        "42": "markdown",
        "43": "markdown",
        "44": "code"
    },
    "source": {
        "0": "import re\nfrom google_play_scraper import app, Sort, reviews\nimport nltk\nimport pandas as pd\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nfrom num2words import num2words\nfrom nltk.corpus import stopwords\nimport numpy as np\nfrom textblob import TextBlob\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "1": "# TASK 1",
        "2": "pd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\npd.set_option('display.width', None)\npd.set_option('display.max_colwidth', None)\napp_df = pd.read_csv('ImpellerApps.csv')\napp_names = list(app_df['app name'])\napp_ids = list(app_df['package name'])\ndf_csv=pd.DataFrame(columns=['Package name','Reviewer name','Review', 'Rating'])\nfor i in app_ids:\n    app_rvws, token = reviews(\n            i, # app's ID, found in app's url\n            lang='en',            # defaults to 'en'\n            country='us',         # defaults to 'us'\n            sort=Sort.NEWEST,     # defaults to Sort.MOST_RELEVANT\n            count=1000             # defaults to 100\n            # , continuation_token=token\n        )\n\n    for r in app_rvws:\n        row_list={'Package name':i,'Reviewer name':r['userName'],'Review':r['content'], 'Rating':r['score']}\n        df_csv = df_csv.append(row_list, ignore_index=True)\n        \ndf_csv.to_csv('eu.e43.impeller.csv')     ",
        "3": "app_df1 = pd.read_csv('TwisterApps.csv')\napp_names1 = list(app_df1['app name'])\napp_ids1 = list(app_df1['package name'])\ndf_csv1=pd.DataFrame(columns=['Package name','Reviewer name','Review', 'Rating'])\nfor i in app_ids1:\n    app_rvws, token = reviews(\n            i, # app's ID, found in app's url\n            lang='en',            # defaults to 'en'\n            country='us',         # defaults to 'us'\n            sort=Sort.NEWEST,     # defaults to Sort.MOST_RELEVANT\n            count=1000             # defaults to 100\n            # , continuation_token=token\n        )\n    \n    for r in app_rvws:\n        row_list={'Package name':i,'Reviewer name':r['userName'],'Review':r['content'], 'Rating':r['score']}\n        df_csv1 = df_csv1.append(row_list, ignore_index=True)\n        \ndf_csv1.to_csv('szelok.app.twister.csv')     ",
        "4": "app_df1 = pd.read_csv('Fwknop2Apps.csv')\napp_names1 = list(app_df1['app name'])\napp_ids1 = list(app_df1['package name'])\ndf_csv1=pd.DataFrame(columns=['Package name','Reviewer name','Review', 'Rating'])\nfor i in app_ids1:\n    app_rvws, token = reviews(\n            i, # app's ID, found in app's url\n            lang='en',            # defaults to 'en'\n            country='us',         # defaults to 'us'\n            sort=Sort.NEWEST,     # defaults to Sort.MOST_RELEVANT\n            count=1000             # defaults to 100\n            # , continuation_token=token\n        )\n    \n    for r in app_rvws:\n        row_list={'Package name':i,'Reviewer name':r['userName'],'Review':r['content'], 'Rating':r['score']}\n        df_csv1 = df_csv1.append(row_list, ignore_index=True)\n        \ndf_csv1.to_csv('org.cipherdyne.fwknop2.csv') ",
        "5": "app_df1 = pd.read_csv('1x1clockApps.csv')\napp_names1 = list(app_df1['app name'])\napp_ids1 = list(app_df1['package name'])\ndf_csv1=pd.DataFrame(columns=['Package name','Reviewer name','Review', 'Rating'])\nfor i in app_ids1:\n    app_rvws, token = reviews(\n            i, # app's ID, found in app's url\n            lang='en',            # defaults to 'en'\n            country='us',         # defaults to 'us'\n            sort=Sort.NEWEST,     # defaults to Sort.MOST_RELEVANT\n            count=1000             # defaults to 100\n            # , continuation_token=token\n        )\n    \n    for r in app_rvws:\n        row_list={'Package name':i,'Reviewer name':r['userName'],'Review':r['content'], 'Rating':r['score']}\n        df_csv1 = df_csv1.append(row_list, ignore_index=True)\n        \ndf_csv1.to_csv('com.github.wakhub.tinyclock.csv') ",
        "6": "app_df1 = pd.read_csv('ColorClockApps.csv')\napp_names1 = list(app_df1['app name'])\napp_ids1 = list(app_df1['package name'])\ndf_csv1=pd.DataFrame(columns=['Package name','Reviewer name','Review', 'Rating'])\nfor i in app_ids1:\n    app_rvws, token = reviews(\n            i, # app's ID, found in app's url\n            lang='en',            # defaults to 'en'\n            country='us',         # defaults to 'us'\n            sort=Sort.NEWEST,     # defaults to Sort.MOST_RELEVANT\n            count=1000             # defaults to 100\n            # , continuation_token=token\n        )\n    \n    for r in app_rvws:\n        row_list={'Package name':i,'Reviewer name':r['userName'],'Review':r['content'], 'Rating':r['score']}\n        df_csv1 = df_csv1.append(row_list, ignore_index=True)\n        \ndf_csv1.to_csv('com.brianco.colorclock.csv') ",
        "7": "# TASK 2",
        "8": "rev_df = pd.read_csv('eu.e43.impeller.csv')\napp_rev = list(rev_df['Review'])\napp_id=list(rev_df['Package name'])\n \nimp_pre_df=pd.DataFrame(columns=['Package name','Review'])\n\nfor r in app_rev:\n    app_desc = str(r)\n# Punctuations\n    # Initialize punctuations string\n    punc = r'''!()-[]{};:'\"...\\,./?@*'''\n    # Remove punctuations in description\n    for ch in app_desc:\n        if ch in punc:\n            app_desc = app_desc.replace(ch, \"\")\n# Emojis         \n    app_desc_sc = re.sub(r'[^\\w\\s]', '', app_desc)\n    # Emoji pattern\n    emoji_pattern = re.compile(\"[\"\n            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               \"]+\", flags=re.UNICODE)\n    # Remove emojis in description\n    emoji_pattern.sub(r'', app_desc_sc)\n# #  remove special characters\n#     app_desc_sc = re.sub(\"[^a-zA-Z0-9]+\", \"\",app_desc_sc)\n#     re.sub('[^A-Za-z0-9]+ ', '', mystring)\n# Number to words\n    app_desc_num=app_desc_sc\n    # Initialize list of words in description\n    word_tokens_num = word_tokenize(app_desc_num)\n    # Iterate list and change numbers to words\n    for i in word_tokens_num:\n        i = re.sub('[^A-Za-z0-9]+', '',i)\n#         print(word_tokens_num)\n        if i.isdigit():\n            app_desc_num = app_desc_num.replace(i, num2words(i))\n# Whitespaces    \n    app_desc_sp = app_desc_num\n    # Remove extra whitespaces using regex string\n    app_desc_sp = re.sub(' {2,}', ' ', app_desc_sp)\n    \n# Lowercase    \n    app_desc_low=app_desc_sp\n    # Initialize list of words in description\n    word_tokens_low = word_tokenize(app_desc_low)\n    # Iterate list and change words to lowercase\n    for i in word_tokens_low:\n        app_desc_low = app_desc_low.replace(i, i.lower())\n# Stop words   \n    #   Initialize set of english stop words\n    stop_words = set(stopwords.words('english'))\n    word_tokens_st = word_tokenize(app_desc_low)\n    #Initialize list with stop words filtered out and initialize new app description string\n    filtered_desc = [w for w in word_tokens_st if not w in stop_words]\n    app_desc_stop=' '.join([str(elem) for elem in filtered_desc])\n    \n# Lemmatize    \n    #   Initialize Lemmatizer object \n    wordnet_lemmatizer = WordNetLemmatizer()\n    app_desc_lem = app_desc_stop\n    word_tokens_lem = word_tokenize(app_desc_lem)\n#   Iterate word tokens and Lemmatize words\n    for w in word_tokens_lem:\n        app_desc_lem = app_desc_lem.replace(w, wordnet_lemmatizer.lemmatize(w))\n# Append to dataframe    \n    row_list={'Package name':app_id[app_rev.index(r)],'Review':app_desc_lem}\n    imp_pre_df = imp_pre_df.append(row_list, ignore_index=True)\n    \nimp_pre_df.sample(16)",
        "9": "rev_df = pd.read_csv('szelok.app.twister.csv')\napp_rev = list(rev_df['Review'])\napp_id=list(rev_df['Package name'])\n \ntwi_pre_df=pd.DataFrame(columns=['Package name','Review'])\n\nfor r in app_rev:\n    app_desc = str(r)\n# Punctuations\n    # Initialize punctuations string\n    punc = r'''!()-[]{};:'\"...\\,./?@*'''\n    # Remove punctuations in description\n    for ch in app_desc:\n        if ch in punc:\n            app_desc = app_desc.replace(ch, \"\")\n# Emojis         \n    app_desc_sc = re.sub(r'[^\\w\\s]', '', app_desc)\n    # Emoji pattern\n    emoji_pattern = re.compile(\"[\"\n            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               \"]+\", flags=re.UNICODE)\n    # Remove emojis in description\n    emoji_pattern.sub(r'', app_desc_sc)\n# Number to words\n    app_desc_num=app_desc_sc\n    # Initialize list of words in description\n    word_tokens_num = word_tokenize(app_desc_num)\n    # Iterate list and change numbers to words\n    for i in word_tokens_num:\n        if i.isdigit():\n            app_desc_num = app_desc_num.replace(i, num2words(i))\n# Whitespaces    \n    app_desc_sp = app_desc_num\n    # Remove extra whitespaces using regex string\n    app_desc_sp = re.sub(' {2,}', ' ', app_desc_sp)\n    \n# Lowercase    \n    app_desc_low=app_desc_sp\n    # Initialize list of words in description\n    word_tokens_low = word_tokenize(app_desc_low)\n    # Iterate list and change words to lowercase\n    for i in word_tokens_low:\n        app_desc_low = app_desc_low.replace(i, i.lower())\n# Stop words   \n    #   Initialize set of english stop words\n    stop_words = set(stopwords.words('english'))\n    word_tokens_st = word_tokenize(app_desc_low)\n    #Initialize list with stop words filtered out and initialize new app description string\n    filtered_desc = [w for w in word_tokens_st if not w in stop_words]\n    app_desc_stop=' '.join([str(elem) for elem in filtered_desc])\n    \n# Lemmatize    \n    #   Initialize Lemmatizer object \n    wordnet_lemmatizer = WordNetLemmatizer()\n    app_desc_lem = app_desc_stop\n    word_tokens_lem = word_tokenize(app_desc_lem)\n#   Iterate word tokens and Lemmatize words\n    for w in word_tokens_lem:\n        app_desc_lem = app_desc_lem.replace(w, wordnet_lemmatizer.lemmatize(w))\n# Append to dataframe    \n    row_list={'Package name':app_id[app_rev.index(r)],'Review':app_desc_lem}\n    twi_pre_df = twi_pre_df.append(row_list, ignore_index=True)\n    \ntwi_pre_df.sample(16)",
        "10": "rev_df = pd.read_csv('org.cipherdyne.fwknop2.csv')\napp_rev = list(rev_df['Review'])\napp_id=list(rev_df['Package name'])\n \nfwk_pre_df=pd.DataFrame(columns=['Package name','Review'])\n\nfor r in app_rev:\n    app_desc = str(r)\n# Punctuations\n    # Initialize punctuations string\n    punc = r'''!()-[]{};:'\"...\\,./?@*'''\n    # Remove punctuations in description\n    for ch in app_desc:\n        if ch in punc:\n            app_desc = app_desc.replace(ch, \"\")\n# Emojis         \n    app_desc_sc = re.sub(r'[^\\w\\s]', '', app_desc)\n    # Emoji pattern\n    emoji_pattern = re.compile(\"[\"\n            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               \"]+\", flags=re.UNICODE)\n    # Remove emojis in description\n    emoji_pattern.sub(r'', app_desc_sc)\n# Number to words\n    app_desc_num=app_desc_sc\n    # Initialize list of words in description\n    word_tokens_num = word_tokenize(app_desc_num)\n    # Iterate list and change numbers to words\n    for i in word_tokens_num:\n        if i.isdigit():\n            app_desc_num = app_desc_num.replace(i, num2words(i))\n# Whitespaces    \n    app_desc_sp = app_desc_num\n    # Remove extra whitespaces using regex string\n    app_desc_sp = re.sub(' {2,}', ' ', app_desc_sp)\n    \n# Lowercase    \n    app_desc_low=app_desc_sp\n    # Initialize list of words in description\n    word_tokens_low = word_tokenize(app_desc_low)\n    # Iterate list and change words to lowercase\n    for i in word_tokens_low:\n        app_desc_low = app_desc_low.replace(i, i.lower())\n# Stop words   \n    #   Initialize set of english stop words\n    stop_words = set(stopwords.words('english'))\n    word_tokens_st = word_tokenize(app_desc_low)\n    #Initialize list with stop words filtered out and initialize new app description string\n    filtered_desc = [w for w in word_tokens_st if not w in stop_words]\n    app_desc_stop=' '.join([str(elem) for elem in filtered_desc])\n    \n# Lemmatize    \n    #   Initialize Lemmatizer object \n    wordnet_lemmatizer = WordNetLemmatizer()\n    app_desc_lem = app_desc_stop\n    word_tokens_lem = word_tokenize(app_desc_lem)\n#   Iterate word tokens and Lemmatize words\n    for w in word_tokens_lem:\n        app_desc_lem = app_desc_lem.replace(w, wordnet_lemmatizer.lemmatize(w))\n# Append to dataframe    \n    row_list={'Package name':app_id[app_rev.index(r)],'Review':app_desc_lem}\n    fwk_pre_df = fwk_pre_df.append(row_list, ignore_index=True)\n    \nfwk_pre_df.sample(16)",
        "11": "rev_df = pd.read_csv('com.github.wakhub.tinyclock.csv')\napp_rev = list(rev_df['Review'])\napp_id=list(rev_df['Package name'])\n \ntc_pre_df=pd.DataFrame(columns=['Package name','Review'])\n\nfor r in app_rev:\n    app_desc = str(r)\n# Punctuations\n    # Initialize punctuations string\n    punc = r'''!()-[]{};:'\"...\\,./?@*'''\n    # Remove punctuations in description\n    for ch in app_desc:\n        if ch in punc:\n            app_desc = app_desc.replace(ch, \"\")\n# Emojis         \n    app_desc_sc = re.sub(r'[^\\w\\s]', '', app_desc)\n    # Emoji pattern\n    emoji_pattern = re.compile(\"[\"\n            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               \"]+\", flags=re.UNICODE)\n    # Remove emojis in description\n    emoji_pattern.sub(r'', app_desc_sc)\n# Number to words\n    app_desc_num=app_desc_sc\n    # Initialize list of words in description\n    word_tokens_num = word_tokenize(app_desc_num)\n    # Iterate list and change numbers to words\n    for i in word_tokens_num:\n        if i.isdigit():\n            app_desc_num = app_desc_num.replace(i, num2words(i))\n# Whitespaces    \n    app_desc_sp = app_desc_num\n    # Remove extra whitespaces using regex string\n    app_desc_sp = re.sub(' {2,}', ' ', app_desc_sp)\n    \n# Lowercase    \n    app_desc_low=app_desc_sp\n    # Initialize list of words in description\n    word_tokens_low = word_tokenize(app_desc_low)\n    # Iterate list and change words to lowercase\n    for i in word_tokens_low:\n        app_desc_low = app_desc_low.replace(i, i.lower())\n# Stop words   \n    #   Initialize set of english stop words\n    stop_words = set(stopwords.words('english'))\n    word_tokens_st = word_tokenize(app_desc_low)\n    #Initialize list with stop words filtered out and initialize new app description string\n    filtered_desc = [w for w in word_tokens_st if not w in stop_words]\n    app_desc_stop=' '.join([str(elem) for elem in filtered_desc])\n    \n# Lemmatize    \n    #   Initialize Lemmatizer object \n    wordnet_lemmatizer = WordNetLemmatizer()\n    app_desc_lem = app_desc_stop\n    word_tokens_lem = word_tokenize(app_desc_lem)\n#   Iterate word tokens and Lemmatize words\n    for w in word_tokens_lem:\n        app_desc_lem = app_desc_lem.replace(w, wordnet_lemmatizer.lemmatize(w))\n# Append to dataframe    \n    row_list={'Package name':app_id[app_rev.index(r)],'Review':app_desc_lem}\n    tc_pre_df = tc_pre_df.append(row_list, ignore_index=True)\n    \ntc_pre_df.sample(16)",
        "12": "rev_df = pd.read_csv('com.brianco.colorclock.csv')\napp_rev = list(rev_df['Review'])\napp_id=list(rev_df['Package name'])\ncc_pre_df=pd.DataFrame(columns=['Package name','Review'])\n\nfor r in app_rev:\n    app_desc = str(r)\n# Punctuations\n    # Initialize punctuations string\n    punc = r'''!()-[]{};:'\"...\\,./?@*'''\n    # Remove punctuations in description\n    for ch in app_desc:\n        if ch in punc:\n            app_desc = app_desc.replace(ch, \"\")\n# Emojis         \n    app_desc_sc = re.sub(r'[^\\w\\s]', '', app_desc)\n    # Emoji pattern\n    emoji_pattern = re.compile(\"[\"\n            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               \"]+\", flags=re.UNICODE)\n    # Remove emojis in description\n    emoji_pattern.sub(r'', app_desc_sc)\n# Number to words\n    app_desc_num=app_desc_sc\n    # Initialize list of words in description\n    word_tokens_num = word_tokenize(app_desc_num)\n    # Iterate list and change numbers to words\n    for i in word_tokens_num:\n        if i.isdigit():\n            app_desc_num = app_desc_num.replace(i, num2words(i))\n# Whitespaces    \n    app_desc_sp = app_desc_num\n    # Remove extra whitespaces using regex string\n    app_desc_sp = re.sub(' {2,}', ' ', app_desc_sp)\n    \n# Lowercase    \n    app_desc_low=app_desc_sp\n    # Initialize list of words in description\n    word_tokens_low = word_tokenize(app_desc_low)\n    # Iterate list and change words to lowercase\n    for i in word_tokens_low:\n        app_desc_low = app_desc_low.replace(i, i.lower())\n# Stop words   \n    #   Initialize set of english stop words\n    stop_words = set(stopwords.words('english'))\n    word_tokens_st = word_tokenize(app_desc_low)\n    #Initialize list with stop words filtered out and initialize new app description string\n    filtered_desc = [w for w in word_tokens_st if not w in stop_words]\n    app_desc_stop=' '.join([str(elem) for elem in filtered_desc])\n    \n# Lemmatize    \n    #   Initialize Lemmatizer object \n    wordnet_lemmatizer = WordNetLemmatizer()\n    app_desc_lem = app_desc_stop\n    word_tokens_lem = word_tokenize(app_desc_lem)\n#   Iterate word tokens and Lemmatize words\n    for w in word_tokens_lem:\n        app_desc_lem = app_desc_lem.replace(w, wordnet_lemmatizer.lemmatize(w))\n# Append to dataframe    \n    row_list={'Package name':app_id[app_rev.index(r)],'Review':app_desc_lem}\n    cc_pre_df = cc_pre_df.append(row_list, ignore_index=True)\n    \ncc_pre_df.sample(16)",
        "13": "# Bonus of TASK 2\n\nI believe the choice of stop words one uses has quite a sizeable impact on the analysis of reviews. This is because reviews come from users of apps who exist within a specific domain or scope so certain words would be more common among the sets of reviews from those users. Thus if one can identify stopwords that are distinct to each app's domain then they can extend the list of stopwords to include the ones they have identified. This seems useful to me because as a result, the pre-processing algorithm used will return an even lighter data set so there are fewer unimportant words passed to whatever topic model one decides to use. Similarly one could replace stopwords that aren't exactly applicable in the given domain in order to retain important information that would otherwise be lost.\n\nThe risk of not using a customized stopwords list is first, losing important data that could drastically impact your analysis and secondly, spending more time than you should on the analysis due to having words in the pre-processed reviews that would be considered stopwords from the standpoint of that specific domain.",
        "14": "# TASK 3",
        "15": "## Textblob Sentiment Analysis",
        "16": "# impeller\nimp_app_rev = list(imp_pre_df['Review'])\nimp_app_id=list(imp_pre_df['Package name'])\n# print(imp_pre_df.head(5))\n# twister\ntwi_app_rev = list(twi_pre_df['Review'])\ntwi_app_id=list(twi_pre_df['Package name'])\ntwi_pre_df.head(5)\n# fwknop2\nfwk_app_rev = list(fwk_pre_df['Review'])\nfwk_app_id=list(fwk_pre_df['Package name'])\n# 1x1 clock\ntc_app_rev = list(tc_pre_df['Review'])\ntc_app_id=list(tc_pre_df['Package name'])\n# color clock\ncc_app_rev = list(cc_pre_df['Review'])\ncc_app_id=list(cc_pre_df['Package name'])\n\nimp_sen_df1=pd.DataFrame(columns=['Package name','Review','Polarity'])\ntwi_sen_df1=pd.DataFrame(columns=['Package name','Review','Polarity'])\nfwk_sen_df1=pd.DataFrame(columns=['Package name','Review','Polarity'])\ntc_sen_df1=pd.DataFrame(columns=['Package name','Review','Polarity'])\ncc_sen_df1=pd.DataFrame(columns=['Package name','Review','Polarity'])\n\nfor st in imp_app_rev:\n    \n    b = TextBlob(str(st))\n    row_list={'Package name':imp_app_id[imp_app_rev.index(st)],'Review':str(st),'Polarity':b.sentiment[0]}\n#     print(row_list)\n    imp_sen_df1 = imp_sen_df1.append(row_list, ignore_index=True)\n\n\nfor st in twi_app_rev:\n    \n    b = TextBlob(str(st))\n    row_list={'Package name':twi_app_id[twi_app_rev.index(st)],'Review':str(st),'Polarity':b.sentiment[0]}\n    twi_sen_df1 = twi_sen_df1.append(row_list, ignore_index=True)\n# print(twi_sen_df1.head(5))\n\n\nfor st in fwk_app_rev:\n#     print(st)\n    b = TextBlob(str(st))\n    row_list={'Package name':fwk_app_id[fwk_app_rev.index(st)],'Review':str(st),'Polarity':b.sentiment[0]}\n    fwk_sen_df1 = fwk_sen_df1.append(row_list, ignore_index=True)\n\n\nfor st in tc_app_rev:\n    b = TextBlob(str(st))\n    row_list={'Package name':tc_app_id[tc_app_rev.index(st)],'Review':str(st),'Polarity':b.sentiment[0]}\n    tc_sen_df1 = tc_sen_df1.append(row_list, ignore_index=True)\n\n\nfor st in cc_app_rev:\n    b = TextBlob(str(st))\n    row_list={'Package name':cc_app_id[cc_app_rev.index(st)],'Review':str(st),'Polarity':b.sentiment[0]}\n    cc_sen_df1 = cc_sen_df1.append(row_list, ignore_index=True)\n\n\nframes = [imp_sen_df1, twi_sen_df1, fwk_sen_df1, tc_sen_df1, cc_sen_df1]\n\nresult = pd.concat(frames)\nresult.sample(250)",
        "17": "## Vader Sentiment Analysis",
        "18": "imp_sen_df2=pd.DataFrame(columns=['Package name','Review','Polarity'])\ntwi_sen_df2=pd.DataFrame(columns=['Package name','Review','Polarity'])\nfwk_sen_df2=pd.DataFrame(columns=['Package name','Review','Polarity'])\ntc_sen_df2=pd.DataFrame(columns=['Package name','Review','Polarity'])\ncc_sen_df2=pd.DataFrame(columns=['Package name','Review','Polarity'])\nanalyzer = SentimentIntensityAnalyzer()\n    \nfor st in imp_app_rev:\n    sentiment_dict = analyzer.polarity_scores(str(st))\n    row_list={'Package name':imp_app_id[imp_app_rev.index(st)],'Review':str(st),'Polarity':sentiment_dict['compound']}\n    imp_sen_df2 = imp_sen_df2.append(row_list, ignore_index=True)\n\n\nfor st in twi_app_rev:\n    \n    sentiment_dict = analyzer.polarity_scores(str(st))\n    row_list={'Package name':twi_app_id[twi_app_rev.index(st)],'Review':str(st),'Polarity':sentiment_dict['compound']}\n    twi_sen_df2 = twi_sen_df2.append(row_list, ignore_index=True)\n\n\nfor st in fwk_app_rev:\n    sentiment_dict = analyzer.polarity_scores(str(st))\n    row_list={'Package name':fwk_app_id[fwk_app_rev.index(st)],'Review':str(st),'Polarity':sentiment_dict['compound']}\n    fwk_sen_df2 = fwk_sen_df2.append(row_list, ignore_index=True)\n\n\nfor st in tc_app_rev:\n    sentiment_dict = analyzer.polarity_scores(str(st))\n    row_list={'Package name':tc_app_id[tc_app_rev.index(st)],'Review':str(st),'Polarity':sentiment_dict['compound']}\n    tc_sen_df2 = tc_sen_df2.append(row_list, ignore_index=True)\n\n\nfor st in cc_app_rev:\n    sentiment_dict = analyzer.polarity_scores(str(st))\n    row_list={'Package name':cc_app_id[cc_app_rev.index(st)],'Review':str(st),'Polarity':sentiment_dict['compound']}\n    cc_sen_df2 = cc_sen_df2.append(row_list, ignore_index=True)\nimp_sen_df2.sample(250)\n\nframes2 = [imp_sen_df2, twi_sen_df2, fwk_sen_df2, tc_sen_df2, cc_sen_df2]\n\nresult2 = pd.concat(frames2)\nresult2.sample(250)",
        "19": "## TASK 3 Part V",
        "20": "imp_com_df=pd.DataFrame(columns=['Package name','Review','Textblob Polarity', 'Vader Polarity'])\nimp_com_df['Package name']=list(imp_sen_df1['Package name'])\nimp_com_df['Review']=list(imp_sen_df1['Review'])\nimp_com_df['Textblob Polarity']=list(imp_sen_df1['Polarity'])\nimp_com_df['Vader Polarity']=list(imp_sen_df2['Polarity'])\nimp_com_df.sample(250)",
        "21": "The sentiments retrieved by Textblob and Vader seem fairly similar for some reviews but upon closer manual inspection with the Columns side by side as shown above I noticed that there are some reviews I have identified as negative such as \"app contains malware created antihuman right sjw cultist avoid cost\" from line 591 for which Textblob sentiment analysis outputs a polarity of 0.285714 which is overall a positive sentiment meanwhile, Vader sentiment analysis outputs a compound value of -0.0516 which is overall a negative sentiment and is a more accurate analysis of the review. My interpretation of the similarity is that some reviews use distinctively positive or negative language so both libraries would output overall positive or negative sentiments. However, the difference can be seen when the review doesn't exactly use distinct negative words such as slang and domain-specific words in this case Vader seems to perform a better analysis of the text. \nI think the best option for review analysis of my apps would be Vader sentiment analysis because the type of language app reviewers are most likely to use is going to be very informal and contain slang and terminology outside everyday English.",
        "22": "# TASK 4",
        "23": "import gensim\nimport gensim.corpora as corpora\nfrom pprint import pprint\nfrom gensim.similarities import MatrixSimilarity\nfrom gensim.matutils import cossim\nimport numpy as np\n\ndef sent_to_words(sentences):\n    for sentence in sentences:\n        yield(gensim.utils.simple_preprocess(str(sentence),\n                                             deacc=True))\n              \ntc_list=tc_sen_df2.loc[tc_sen_df2['Package name'] == 'com.github.wakhub.tinyclock']['Review'].values.tolist()\ntc_words = list(sent_to_words(tc_list))\n# Create Dictionary\nid2word = corpora.Dictionary(tc_words)\n# Create Corpus\ntexts = tc_words\n# Term Document Frequency\ncorpus = [id2word.doc2bow(text) for text in texts]\n\n# number of topics\nnum_topics = 10\n# Build LDA model\nlda_model = gensim.models.LdaMulticore(corpus=corpus,\n                                       id2word=id2word,\n                                       num_topics=num_topics\n                                      )\n# Print the Keyword in the 10 topics\npprint(lda_model.print_topics())",
        "24": "## TASK 4 Part III\n\nThe review topics are very closely related to the software features we manually extracted in assignment 2. They are also very similar to the automated n-grams we extracted in assignment 3. We can take the 4th and 5th topics from the list above, for example, the words clock (excluding app because it appears in topic 1) and time have the most contributions to those topics and one of the features extracted in assignment 2 was 1x1 clock, in assignment 3 the bi-grams extracted were small 1x1, 1x1 clock, and clock widget and the tri-grams were small 1x1 clock and 1x1 clock widget. These extracted n-grams are also very similar to the topics extracted above.",
        "25": "avr_list=tc_sen_df2.loc[tc_sen_df2['Package name'] == 'com.avrapps.clockwidget']['Review'].values.tolist()\navr_words = list(sent_to_words(avr_list))\n# Create Dictionary\nid2word = corpora.Dictionary(avr_words)\n# Create Corpus\ntexts = avr_words\n# Term Document Frequency\ncorpus_a = [id2word.doc2bow(text) for text in texts]\n\n# number of topics\nnum_topics_c = 5\n# Build LDA model\nlda_model_avr = gensim.models.LdaMulticore(corpus=corpus_a,\n                                       id2word=id2word,\n                                       num_topics=num_topics_c)\n# Print the Keyword in the 5 topics\npprint(lda_model_avr.print_topics())\n",
        "26": "# tc_list=tc_sen_df2.loc[tc_sen_df2['Package name'] == 'com.github.wakhub.tinyclock']['Review'].values.tolist()\n\nnice_list=tc_sen_df2.loc[tc_sen_df2['Package name'] == 'com.nullium.nicesimpleclock']['Review'].values.tolist()\n\n\nnice_words = list(sent_to_words(nice_list))\n# Create Dictionary\nid2word = corpora.Dictionary(nice_words)\n# Create Corpus\ntexts = nice_words\n# Term Document Frequency\ncorpus_n = [id2word.doc2bow(text) for text in texts]\n\n# number of topics\nnum_topics_c = 5\n# Build LDA model\nlda_model_nice = gensim.models.LdaMulticore(corpus=corpus_n,\n                                       id2word=id2word,\n                                       num_topics=num_topics_c)\n# Print the Keyword in the 5 topics\npprint(lda_model_nice.print_topics())\n\n",
        "27": "# tc_list=tc_sen_df2.loc[tc_sen_df2['Package name'] == 'com.github.wakhub.tinyclock']['Review'].values.tolist()\n\nprime_list=tc_sen_df2.loc[tc_sen_df2['Package name'] == 'com.primedev.digitalclock.widget.free']['Review'].values.tolist()\n\n\nprime_words = list(sent_to_words(prime_list))\n# Create Dictionary\nid2word = corpora.Dictionary(prime_words)\n# Create Corpus\ntexts = prime_words\n# Term Document Frequency\ncorpus_p = [id2word.doc2bow(text) for text in texts]\n# number of topics\nnum_topics_c = 5\n# Build LDA model\nlda_model_p = gensim.models.LdaMulticore(corpus=corpus_p,\n                                       id2word=id2word,\n                                       num_topics=num_topics_c\n                                      )\n# Print the Keyword in the 5 topics\npprint(lda_model_p.print_topics())\n",
        "28": "# tc_list=tc_sen_df2.loc[tc_sen_df2['Package name'] == 'com.github.wakhub.tinyclock']['Review'].values.tolist()\n\nsim_list=tc_sen_df2.loc[tc_sen_df2['Package name'] == 'com.chibatching.worldclockwidget']['Review'].values.tolist()\n\n\nsim_words = list(sent_to_words(sim_list))\n# Create Dictionary\nid2word = corpora.Dictionary(sim_words)\n# Create Corpus\ntexts = sim_words\n# Term Document Frequency\ncorpus_s = [id2word.doc2bow(text) for text in texts]\n# number of topics\nnum_topics_c = 5\n# Build LDA model\nlda_model_s = gensim.models.LdaMulticore(corpus=corpus_s,\n                                       id2word=id2word,\n                                       num_topics=num_topics_c)\n# Print the Keyword in the 5 topics\n# matrix = np.zeros(shape=(len(lda_model_avr[corpus]), len(lda_model_avr[corpus])),dtype=float)\npprint(lda_model_s.print_topics())\n",
        "29": "## TASK 4 Part V  ",
        "30": "## 1x1 clock Vs AVR-Apps-Digital Clock Widget",
        "31": "hits=0\nlosses=0\n\nfor i in range(0, len(lda_model[corpus])):\n    for j in range(0, len(lda_model_avr[corpus_a])):\n        doc_1 = lda_model[corpus][i]\n        doc_2 = lda_model_avr[corpus_a][j]\n        if (cossim(doc_1, doc_2)>=0.5):\n            hits+=1\n            if(i<5 and j<10):\n                print('Cosine_Similarity_'+str(i)+'_'+str(j)+':',cossim(doc_1, doc_2)) \n        else:\n            losses+=1\nprint('')\nprint(\"Hits:\", hits)\nprint(\"Losses:\", losses)\nprint(\"Hit Ratio:\",str('{0:.2f}'.format((hits/(hits+losses)*100)))+'%')\n\n#         print('Cosine_Similarity_'+str(i)+'_'+str(j)+':',cossim(doc_1, doc_2))\n       \n    ",
        "32": "## 1x1 clock Vs Nice Simple Clock (Widget)",
        "33": "hits=0\nlosses=0\n\nfor i in range(0, len(lda_model[corpus])):\n    for j in range(0, len(lda_model_nice[corpus_n])):\n        doc_1 = lda_model[corpus][i]\n        doc_2 = lda_model_nice[corpus_n][j]\n        if (cossim(doc_1, doc_2)>=0.5):\n            hits+=1\n            if(i<5 and j<10):\n                print('Cosine_Similarity_'+str(i)+'_'+str(j)+':',cossim(doc_1, doc_2)) \n        else:\n            losses+=1\nprint('')\nprint(\"Hits:\", hits)\nprint(\"Losses:\", losses)\nprint(\"Hit Ratio:\",str('{0:.2f}'.format((hits/(hits+losses)*100)))+'%')",
        "34": "## 1x1 clock Vs Prime Dev Studio- Digital Clock Widget",
        "35": "hits=0\nlosses=0\n\nfor i in range(0, len(lda_model[corpus])):\n    for j in range(0, len(lda_model_p[corpus_p])):\n        doc_1 = lda_model[corpus][i]\n        doc_2 = lda_model_p[corpus_p][j]\n        if (cossim(doc_1, doc_2)>=0.5):\n            hits+=1\n            if(i<5 and j<10):\n                print('Cosine_Similarity_'+str(i)+'_'+str(j)+':',cossim(doc_1, doc_2))\n        else:\n            losses+=1\nprint('')\nprint(\"Hits:\", hits)\nprint(\"Losses:\", losses)\nprint(\"Hit Ratio:\",str('{0:.2f}'.format((hits/(hits+losses)*100)))+'%')",
        "36": "## 1x1 clock Vs Prime Simple World Clock Widget",
        "37": "hits=0\nlosses=0\n\nfor i in range(0, len(lda_model[corpus])):\n    for j in range(0, len(lda_model_s[corpus_s])):\n        doc_1 = lda_model[corpus][i]\n        doc_2 = lda_model_s[corpus_s][j]\n        if (cossim(doc_1, doc_2)>=0.5):\n            hits+=1\n            if(i<5 and j<10):\n                print('Cosine_Similarity_'+str(i)+'_'+str(j)+':',cossim(doc_1, doc_2))\n        else:\n            losses+=1\nprint('')\nprint(\"Hits:\", hits)\nprint(\"Losses:\", losses)\nprint(\"Hit Ratio:\",str('{0:.2f}'.format((hits/(hits+losses)*100)))+'%')",
        "38": "I mainly used Cosine Similarity to compare the topics between my Assigned app and the chosen similar and competitor apps. Then I used the criteria of a cosine similarity of over 0.5 to identify reviews that share similar topics and calculated hits and losses as well as the percentage of hits to the total reviews.",
        "39": "# Bonus of TASK 4",
        "40": "def format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=tc_list):\n    # Init output\n    sent_topics_df = pd.DataFrame()\n\n    # Get main topic in each document\n    for i, row in enumerate(ldamodel[corpus]):\n        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n        # Get the Dominant topic, Perc Contribution and Keywords for each document\n        for j, (topic_num, prop_topic) in enumerate(row):\n            if j == 0:  # => dominant topic\n                wp = ldamodel.show_topic(topic_num)\n                topic_keywords = \", \".join([word for word, prop in wp])\n                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n            else:\n                break\n    sent_topics_df.columns = ['Dominant_Topic', 'Percent_Contribution', 'Topic_Keywords']\n\n    # Add original text to the end of the output\n    contents = pd.Series(texts)\n    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n    return(sent_topics_df)\n\n\ndf_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=tc_list)\n\n# Format\ndf_dominant_topic = df_topic_sents_keywords.reset_index(drop=True)\ndf_dominant_topic.columns = ['Dominant_Topic', 'Topic_Percent_Contribution', 'Keywords', 'Review']\n\n# Show\ndf_dominant_topic.head(36)\n# print(len(lda_model[corpus]))",
        "41": "## Bonus of TASK 4 Part II\n\n### High Priority Reviews\n1. REV-0: \"It was a learning curve. The one square cut off the p.m. touch and hold the \n    square, it will be highlighted and then you can pull down on it to make it \n    longer. That was too long for me so I put it back up to the one square and \n    then I pulled it sideways to make it longer, that was better. If you're \n    going to change the color of the text or background, you put the word of \n    the color you want, press ok, bring up the change color selection again and \n    than press ok again, then your selection will be entered.\" by James Stutzman. \n\n\n2. REV-2: \"I'm disappointed in this app. It looks nice on here, but it doesn't look \n    nice on my phone. I have no idea how in the hell this app got 4 stars. It's \n    fairly worthless. How do you pick colors for anything without an \n    explanation of what code is for what color??? This app is STUPID!!!!!!!!!!\" by Amber Adams.\n\n\n3. REV-3: \"Time does not fit into the widget\" by Elle H.\n\n\n4. REV-5: \"Terrible. Display cuts of part of the time shown. Deleted.\" by A Google user.\n\n\n5. REV-11: \"Virtually perfect. Font size adjustment, and directly launching the clock app would top it off.\" by Rob Saunders.\n\n\n6. REV-14: \"How to know the colour codes aabbffgg what is this? I am trying to put background colours but only 2  3 colours can b applicable in pics they are showing more colours...how to get them?are there any colour listings?\" by Siddharth Shah ss.\n\n\n7. REV-10: \"When I tap into Clock App, they're many apps for me to select, but without my Clock itself. Please include as it should be the obvious app people want to associate with thid widget.\" by Liau Kok Meng.\n\n\n8. REV-15: \"Wanted to display UTC, but it's linked to my system time, and it changed my system time.\" by Joel Marion.\n\n\n9. REV-19: \"The time font is too big and gets cut off. So instead of reading 11:15, I get 1:15. Which sucks. Other than that the app is solid. Please advise on changing font size and I'll rate it again.\" by Matt.\n\n\n10. REV-27: \"5 if you remove time and date settings . no one ever uses them. Straight to alarms on tap please.\" by neil murray.\n\nMy Thinking for the high priority reviews selected above that these reviews contain either bugs informally reported by customers or general ideas for improving the user experience. For example, if you take a loost at the first review REV-0 the review mainly talks about the shape of the widget anf how it the borders cut off the time in the pm so it seems to me that the clock shows times during the day well because they are single digit such as 8:30 but times in the pm would be double digit such as 20:22.\n",
        "42": "# TASK 5\n\n\n## TASK 5 Part II\n1. Display Cuts off the time shown when the time has more than 3 digits e.g. 11:15 shows 1:15\n2. Provide user-friendly way to change color preferences\n\nI came to this conclusion by analyzing the dominant topics in all the reviews as well as their ratings for my chosen assigned app. At least 6 out of 36(over 15%) of the reviews mention the fit of the time on the display for example REV-0 details that the border needs to be adjusted in order to display the full time which is bad for the overall user experience. Secondly, about another 5 reviews mention issues with being able to modify the font size and color which also has an adverse effect on the user experience.",
        "43": "## Bonus of TASK 5\n1. https://github.com/wakhub/tinyclock/issues/3#issue-1197525051\n2. https://github.com/wakhub/tinyclock/issues/4#issue-1197548619",
        "44": ""
    },
    "rank": {
        "0": 0,
        "1": 0,
        "2": 1,
        "3": 2,
        "4": 3,
        "5": 4,
        "6": 5,
        "7": 1,
        "8": 6,
        "9": 7,
        "10": 8,
        "11": 9,
        "12": 10,
        "13": 2,
        "14": 3,
        "15": 4,
        "16": 11,
        "17": 5,
        "18": 12,
        "19": 6,
        "20": 13,
        "21": 7,
        "22": 8,
        "23": 14,
        "24": 9,
        "25": 15,
        "26": 16,
        "27": 17,
        "28": 18,
        "29": 10,
        "30": 11,
        "31": 19,
        "32": 12,
        "33": 20,
        "34": 13,
        "35": 21,
        "36": 14,
        "37": 22,
        "38": 15,
        "39": 16,
        "40": 23,
        "41": 17,
        "42": 18,
        "43": 19,
        "44": 24
    },
    "pred": {
        "0": 0.04,
        "1": 0.05,
        "2": 0.08,
        "3": 0.12,
        "4": 0.16,
        "5": 0.2,
        "6": 0.24,
        "7": 0.1,
        "8": 0.28,
        "9": 0.32,
        "10": 0.36,
        "11": 0.4,
        "12": 0.44,
        "13": 0.15,
        "14": 0.2,
        "15": 0.25,
        "16": 0.48,
        "17": 0.3,
        "18": 0.52,
        "19": 0.35,
        "20": 0.56,
        "21": 0.4,
        "22": 0.45,
        "23": 0.6,
        "24": 0.5,
        "25": 0.64,
        "26": 0.68,
        "27": 0.72,
        "28": 0.76,
        "29": 0.55,
        "30": 0.6,
        "31": 0.8,
        "32": 0.65,
        "33": 0.84,
        "34": 0.7,
        "35": 0.88,
        "36": 0.75,
        "37": 0.92,
        "38": 0.8,
        "39": 0.85,
        "40": 0.96,
        "41": 0.9,
        "42": 0.95,
        "43": 1.0,
        "44": 1.0
    },
    "pct_rank": {
        "0": 0,
        "1": 0,
        "2": 0,
        "3": 0,
        "4": 0,
        "5": 0,
        "6": 0,
        "7": 0,
        "8": 0,
        "9": 0,
        "10": 0,
        "11": 0,
        "12": 0,
        "13": 0,
        "14": 0,
        "15": 0,
        "16": 0,
        "17": 0,
        "18": 0,
        "19": 0,
        "20": 0,
        "21": 0,
        "22": 0,
        "23": 0,
        "24": 0,
        "25": 0,
        "26": 0,
        "27": 0,
        "28": 0,
        "29": 0,
        "30": 0,
        "31": 0,
        "32": 0,
        "33": 0,
        "34": 0,
        "35": 0,
        "36": 0,
        "37": 0,
        "38": 0,
        "39": 0,
        "40": 0,
        "41": 0,
        "42": 0,
        "43": 0,
        "44": 0
    }
}